{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-scottish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "embedded-draft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7f72a5158c40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import copy\n",
    "import json\n",
    "import importlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import (\n",
    "    show_sbs,\n",
    "    load_config,\n",
    "    _print,\n",
    ")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "champion-ambassador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f71f8a63030>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "flying-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_NAME = \"isic2018_unet.yaml\"\n",
    "CONFIG_FILE_PATH = os.path.join(\"./configs\", CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "studied-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_FILE_PATH)\n",
    "# _print(\"Config:\", \"info_underline\")\n",
    "# print(json.dumps(config, indent=2))\n",
    "# print(20*\"~-\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-hydrogen",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "classical-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img_pred_gt(img, pred, gt, figsize=(8,4)):\n",
    "    _, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "    x = img.squeeze().permute([1, 2, 0]).to('cpu').numpy().astype(np.float)\n",
    "    y = gt.squeeze().detach().to('cpu').numpy().astype(np.float)\n",
    "    p = pred.squeeze().detach().to('cpu').numpy().astype(np.float)\n",
    "    axs[0].imshow(x); axs[0].set_title('image')\n",
    "    axs[1].imshow(p); axs[1].set_title('pred')\n",
    "    axs[2].imshow(y); axs[2].set_title('gt')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_loss_curves(tr, vl):\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"DiceLoss\")\n",
    "    plt.plot(range(len(tr)), tr, 'r')\n",
    "    plt.plot(range(len(vl)), vl, 'b')\n",
    "    plt.legend(['Traning', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-gender",
   "metadata": {},
   "source": [
    "### dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "radical-sunglasses",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/staff/azad/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/staff/azad/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision in /home/staff/azad/.local/lib/python3.8/site-packages (0.10.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "Requirement already satisfied: requests in /work/scratch/azad/anaconda3/lib/python3.8/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /home/staff/azad/.local/lib/python3.8/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /work/scratch/azad/anaconda3/lib/python3.8/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: torch==1.12.1 in /work/scratch/azad/anaconda3/lib/python3.8/site-packages (from torchvision) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /home/staff/azad/.local/lib/python3.8/site-packages (from torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /work/scratch/azad/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /work/scratch/azad/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /work/scratch/azad/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /work/scratch/azad/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (2.0.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/staff/azad/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/home/staff/azad/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torchvision 0.10.0\n",
      "    Uninstalling torchvision-0.10.0:\n",
      "      Successfully uninstalled torchvision-0.10.0\n",
      "Successfully installed torchvision-0.13.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/staff/azad/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/staff/azad/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/staff/azad/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update torchvision\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.isic import ISIC2018TrainingDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- params --------------------\n",
    "INPUT_SIZE = config['dataset']['input_size']\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "\n",
    "# ----------------- transforms ------------------\n",
    "# transform for image\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(\n",
    "        size=[INPUT_SIZE, INPUT_SIZE], \n",
    "        interpolation=transforms.functional.InterpolationMode.BILINEAR\n",
    "    ),\n",
    "])\n",
    "# transform for mask\n",
    "msk_transform = transforms.Compose([\n",
    "    transforms.Resize(\n",
    "        size=[INPUT_SIZE, INPUT_SIZE], \n",
    "        interpolation=transforms.functional.InterpolationMode.NEAREST\n",
    "    ),\n",
    "])\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "\n",
    "# ----------------- dataset --------------------\n",
    "# preparing training dataset\n",
    "train_dataset = ISIC2018TrainingDataset(\n",
    "    img_transform=img_transform,\n",
    "    msk_transform=msk_transform\n",
    ")\n",
    "\n",
    "# We consider 1815 samples for training, 259 samples for validation and 520 samples for testing\n",
    "# !cat ~/deeplearning/skin/Prepare_ISIC2018.py\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "\n",
    "# split indices to: -> train, validation, and test\n",
    "tr_indices = indices[0:1815]\n",
    "vl_indices = indices[1815:1815+259]\n",
    "te_indices = indices[1815+259:2594]\n",
    "\n",
    "# create new datasets from train dataset as training, validation, and test\n",
    "tr_dataset = Subset(train_dataset, tr_indices)\n",
    "vl_dataset = Subset(train_dataset, vl_indices)\n",
    "te_dataset = Subset(train_dataset, te_indices)\n",
    "print(f\"Length of trainig_dataset:\\t{len(tr_dataset)}\")\n",
    "print(f\"Length of validation_dataset:\\t{len(vl_dataset)}\")\n",
    "print(f\"Length of test_dataset:\\t\\t{len(te_dataset)}\")\n",
    "\n",
    "\n",
    "# prepare train dataloader\n",
    "tr_dataloader = DataLoader(tr_dataset, **config['data_loader']['train'])\n",
    "\n",
    "# prepare validation dataloader\n",
    "vl_dataloader = DataLoader(vl_dataset, **config['data_loader']['validation'])\n",
    "\n",
    "# prepare test dataloader\n",
    "te_dataloader = DataLoader(te_dataset, **config['data_loader']['test'])\n",
    "\n",
    "# -------------- test -----------------\n",
    "# test and visualize the input data\n",
    "for img, msk in tr_dataloader:\n",
    "    print(\"Training\")\n",
    "    show_sbs(img[0], msk[0])\n",
    "    break\n",
    "    \n",
    "for img, msk in vl_dataloader:\n",
    "    print(\"Validation\")\n",
    "    show_sbs(img[1], msk[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-perspective",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Torch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-stable",
   "metadata": {},
   "source": [
    "### model and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchmetrics\n",
    "from models.unet import Unet\n",
    "\n",
    "from torch.optim import (\n",
    "  Adam\n",
    ")\n",
    "\n",
    "from losses import (\n",
    "    DiceLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-institution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricCollection(\n",
    "    [\n",
    "        MetricCollection([\n",
    "            Accuracy(num_classes=3, average='macro'),\n",
    "            Precision(num_classes=3, average='macro')\n",
    "        ], postfix='_macro'),\n",
    "        MetricCollection([\n",
    "            Accuracy(num_classes=3, average='micro'),\n",
    "            Precision(num_classes=3, average='micro')\n",
    "        ], postfix='_micro'),\n",
    "    ], \n",
    "    prefix='valmetrics/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-crazy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-malta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = torchmetrics.MetricCollection(\n",
    "    [\n",
    "        torchmetrics.MetricCollection(\n",
    "            [\n",
    "                torchmetrics.F1Score(num_classes=2, threshold=0.5, average='micro'),\n",
    "                torchmetrics.Accuracy(num_classes=2, threshold=0.5, average='micro'),\n",
    "                torchmetrics.Dice(num_classes=2, threshold=0.5, average='micro'),\n",
    "            ], postfix='_micro'),\n",
    "        torchmetrics.MetricCollection(\n",
    "            [\n",
    "                torchmetrics.F1Score(num_classes=2, threshold=0.5, average='macro'),\n",
    "                torchmetrics.Accuracy(num_classes=2, threshold=0.5, average='macro'),\n",
    "                torchmetrics.Dice(num_classes=2, threshold=0.5, average='macro'),\n",
    "            ], postfix='_macro'),\n",
    "    ],\n",
    "    prefix='train_metrics/'\n",
    ")\n",
    "\n",
    "# train_metrics\n",
    "train_metrics = metrics.clone(prefix='train_metrics/')\n",
    "\n",
    "# valid_metrics\n",
    "valid_metrics = metrics.clone(prefix='valid_metrics/')\n",
    "\n",
    "# test_metrics\n",
    "test_metrics = metrics.clone(prefix='test_metrics/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-anime",
   "metadata": {},
   "source": [
    "## validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, vl_dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "#         print('validating...')\n",
    "        \n",
    "        # calculate metrics per batch\n",
    "        evaluator = valid_metrics\n",
    "        \n",
    "        losses = []\n",
    "        cnt = 0.\n",
    "        iterator = tqdm(enumerate(vl_dataloader), leave=None)\n",
    "        for batch, (imgs, msks) in iterator:\n",
    "            cnt += msks.shape[0]\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            msks = msks.to(device)\n",
    "            \n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, msks)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            metrics = evaluator(preds, msks)\n",
    "            _cml = f\"curr_mean-loss:{np.sum(losses)/cnt:0.5f}\"\n",
    "            _bl = f\"batch-loss:{losses[-1]/msks.shape[0]:0.5f}\"\n",
    "            iterator.set_description(f\"Validation) batch:{batch+1:04d} -> {_cml}, {_bl}\")\n",
    "        \n",
    "        # print the final results\n",
    "        loss = np.sum(losses)/cnt\n",
    "        t_m = evaluator.compute()\n",
    "        _ams = ', '.join([f'{k}: {v:0.5f}' for k,v in t_m.items()])\n",
    "        iterator.set_description(f\"Validation_result (on all val_data): {_ams}\")\n",
    "        \n",
    "        evaluator.reset()\n",
    "    \n",
    "    return t_m, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-muscle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-causing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-delay",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = torchmetrics.MetricCollection(\n",
    "    [\n",
    "        torchmetrics.Accuracy().to(device),\n",
    "        torchmetrics.Precision().to(device),\n",
    "        torchmetrics.Specificity().to(device),\n",
    "        torchmetrics.Recall().to(device),\n",
    "        torchmetrics.Dice().to(device),\n",
    "        torchmetrics.F1Score(\n",
    "#             reduce='macro', \n",
    "#             num_classes=2, \n",
    "#             multiclass=True,\n",
    "#             threshold=0.5,\n",
    "#             reduction='elementwise_mean',\n",
    "#             average='macro'\n",
    "        ).to(device),\n",
    "        torchmetrics.JaccardIndex(\n",
    "            num_classes=2, \n",
    "            ignore_index=None, \n",
    "            absent_score=0.0, \n",
    "#             threshold=0.5, \n",
    "            multilabel=False, \n",
    "            reduction='elementwise_mean', \n",
    "            compute_on_step=None).to(device),\n",
    "#         torchmetrics.Accuracy(threshold=0.5, num_classes=1).to(device),\n",
    "#         torchmetrics.Dice(num_classes=2, threshold=0.5, average='macro').to(device),\n",
    "    ],\n",
    "    prefix='train/')\n",
    "\n",
    "# train_metrics\n",
    "train_metrics = metrics.clone()\n",
    "\n",
    "\n",
    "evaluator = train_metrics\n",
    "msks_int = msks>0.5\n",
    "\n",
    "for p, m in zip(preds, msks_int):\n",
    "    evaluator(p.reshape(1, -1).squeeze(), m.reshape(1, -1).squeeze())\n",
    "\n",
    "res = evaluator.compute()\n",
    "for k,v in res.items():\n",
    "    res[k] = v.item()\n",
    "print(json.dumps(res, indent=4))\n",
    "evaluator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "show_img_pred_gt(imgs[idx], preds[idx]>0, msks[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-realtor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-corruption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-wholesale",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_prms = config['training']\n",
    "EPOCHS = tr_prms['epochs']\n",
    "\n",
    "criterion = globals()[tr_prms['criterion']['name']](**tr_prms['criterion']['params'])\n",
    "optimizer = globals()[tr_prms['optimizer']['name']](model.parameters(), **tr_prms['optimizer']['params'])\n",
    "# optimizer = optim.RMSprop(Net.parameters(), lr= float(config['lr']), weight_decay=1e-8, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# calculate metrics per batch\n",
    "\n",
    "evaluator = train_metrics\n",
    "\n",
    "\n",
    "epochs_info = []\n",
    "best_vl_loss = np.Inf\n",
    "epoch_tqdm = tqdm(range(EPOCHS), nrows=2)\n",
    "for epoch in epoch_tqdm:\n",
    "    model.train()\n",
    "\n",
    "    evaluator.reset()\n",
    "    tr_iterator = tqdm(enumerate(tr_dataloader), leave=None)\n",
    "    tr_losses = []\n",
    "    cnt = 0\n",
    "    for batch, (imgs, msks) in tr_iterator:\n",
    "        imgs = imgs.to(device)\n",
    "        msks = msks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, msks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        msks_ = torch.argmax(msks.squeeze(), dim=1)\n",
    "        tr_metrics = evaluator(preds, msks_)\n",
    "        cnt += imgs.shape[0]\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "        # write details for each training batch\n",
    "        _cml = f\"curr_mean-loss:{np.sum(tr_losses)/cnt:0.5f}\"\n",
    "        _bl = f\"mean_batch-loss:{tr_losses[-1]/imgs.shape[0]:0.5f}\"\n",
    "        tr_iterator.set_description(f\"Training) batch:{batch+1:04d} -> {_cml}, {_bl}\")\n",
    "\n",
    "\n",
    "#             if cnt>150: break\n",
    "\n",
    "    # validate model\n",
    "#         tr_iterator.set_description(f\"Validation... (tr-loss:{np.sum(tr_losses)/cnt:0.5f})\")\n",
    "    vl_metrics, vl_loss = validate(model, criterion, vl_dataloader)\n",
    "    if vl_loss < best_vl_loss:\n",
    "        # find a better model\n",
    "        best_model = model\n",
    "        best_vl_loss = vl_loss\n",
    "\n",
    "    # print the final results\n",
    "    epoch_info = {\n",
    "        'tr_loss': np.sum(tr_losses)/cnt,\n",
    "        'vl_loss': vl_loss,\n",
    "        'tr_metrics': evaluator.compute(),\n",
    "        'vl_metrics': vl_metrics\n",
    "    }\n",
    "    epochs_info.append(epoch_info)\n",
    "\n",
    "    # write details for this epoch\n",
    "    _bvl = f'best_vl-loss:{best_vl_loss:0.5f}'\n",
    "    _ltl = f\"last_tr-loss:{epoch_info['tr_loss']:0.5f}\"\n",
    "    _tr_ams = ', '.join([f'tr_{k}: {v:0.4f}' for k,v in epoch_info['tr_metrics'].items()])\n",
    "    _vl_ams = ', '.join([f'vl_{k}: {v:0.4f}' for k,v in epoch_info['vl_metrics'].items()])\n",
    "\n",
    "    epoch_tqdm.set_description(f\"Epoch:{epoch+1}/{EPOCHS} -> {_bvl}, {_ltl}, {_vl_ams}, {_tr_ams}\")\n",
    "\n",
    "    evaluator.reset()\n",
    "\n",
    "#         if cnt>5: break\n",
    "\n",
    "# save final results\n",
    "res = {\n",
    "    'id': save_file_id,\n",
    "    'config': config,\n",
    "    'epochs_info': epochs_info\n",
    "}\n",
    "# fn = f\"{save_file_id+'_' if save_file_id else ''}result.json\"\n",
    "# fp = os.path.join(config['model']['save_dir'],fn)\n",
    "# with open(fp, \"w\") as write_file:\n",
    "#     json.dump(res, write_file, indent=4)\n",
    "\n",
    "# # save model's state_dict\n",
    "# fn = f\"{save_file_id if save_file_id else 'model'}_state_dict.pt\"\n",
    "# fp = os.path.join(config['model']['save_dir'],fn)\n",
    "# torch.save(model.state_dict(), fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-property",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-lecture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-runner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-partner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-material",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-distinction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sacred-repeat",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, \n",
    "    device, \n",
    "    tr_dataloader,\n",
    "    vl_dataloader,\n",
    "    config,\n",
    "    save_dir='./',\n",
    "    save_file_id=None,\n",
    "):\n",
    "    tr_prms = config['training']\n",
    "    EPOCHS = tr_prms['epochs']\n",
    "  \n",
    "    criterion = globals()[tr_prms['criterion']['name']](**tr_prms['criterion']['params'])\n",
    "    optimizer = globals()[tr_prms['optimizer']['name']](model.parameters(), **tr_prms['optimizer']['params'])\n",
    "    # optimizer = optim.RMSprop(Net.parameters(), lr= float(config['lr']), weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    \n",
    "    # calculate metrics per batch\n",
    "    \n",
    "    evaluator = train_metrics\n",
    "    \n",
    "    \n",
    "    epochs_info = []\n",
    "    best_vl_loss = np.Inf\n",
    "    epoch_tqdm = tqdm(range(EPOCHS), nrows=2)\n",
    "    for epoch in epoch_tqdm:\n",
    "        model.train()\n",
    "        \n",
    "        evaluator.reset()\n",
    "        tr_iterator = tqdm(enumerate(tr_dataloader), leave=None)\n",
    "        tr_losses = []\n",
    "        cnt = 0\n",
    "        for batch, (imgs, msks) in tr_iterator:\n",
    "            imgs = imgs.to(device)\n",
    "            msks = msks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, msks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            msks_ = torch.argmax(msks.squeeze(), dim=1)\n",
    "            tr_metrics = evaluator(preds, msks_)\n",
    "            cnt += imgs.shape[0]\n",
    "            tr_losses.append(loss.item())\n",
    "            \n",
    "            # write details for each training batch\n",
    "            _cml = f\"curr_mean-loss:{np.sum(tr_losses)/cnt:0.5f}\"\n",
    "            _bl = f\"mean_batch-loss:{tr_losses[-1]/imgs.shape[0]:0.5f}\"\n",
    "            tr_iterator.set_description(f\"Training) batch:{batch+1:04d} -> {_cml}, {_bl}\")\n",
    "\n",
    "            \n",
    "#             if cnt>150: break\n",
    "            \n",
    "        # validate model\n",
    "#         tr_iterator.set_description(f\"Validation... (tr-loss:{np.sum(tr_losses)/cnt:0.5f})\")\n",
    "        vl_metrics, vl_loss = validate(model, criterion, vl_dataloader)\n",
    "        if vl_loss < best_vl_loss:\n",
    "            # find a better model\n",
    "            best_model = model\n",
    "            best_vl_loss = vl_loss\n",
    "        \n",
    "        # print the final results\n",
    "        epoch_info = {\n",
    "            'tr_loss': np.sum(tr_losses)/cnt,\n",
    "            'vl_loss': vl_loss,\n",
    "            'tr_metrics': evaluator.compute(),\n",
    "            'vl_metrics': vl_metrics\n",
    "        }\n",
    "        epochs_info.append(epoch_info)\n",
    "        \n",
    "        # write details for this epoch\n",
    "        _bvl = f'best_vl-loss:{best_vl_loss:0.5f}'\n",
    "        _ltl = f\"last_tr-loss:{epoch_info['tr_loss']:0.5f}\"\n",
    "        _tr_ams = ', '.join([f'tr_{k}: {v:0.4f}' for k,v in epoch_info['tr_metrics'].items()])\n",
    "        _vl_ams = ', '.join([f'vl_{k}: {v:0.4f}' for k,v in epoch_info['vl_metrics'].items()])\n",
    "        \n",
    "        epoch_tqdm.set_description(f\"Epoch:{epoch+1}/{EPOCHS} -> {_bvl}, {_ltl}, {_vl_ams}, {_tr_ams}\")\n",
    "        \n",
    "        evaluator.reset()\n",
    "        \n",
    "#         if cnt>5: break\n",
    "        \n",
    "    # save final results\n",
    "    res = {\n",
    "        'id': save_file_id,\n",
    "        'config': config,\n",
    "        'epochs_info': epochs_info\n",
    "    }\n",
    "    fn = f\"{save_file_id+'_' if save_file_id else ''}result.json\"\n",
    "    fp = os.path.join(config['model']['save_dir'],fn)\n",
    "    with open(fp, \"w\") as write_file:\n",
    "        json.dump(res, write_file, indent=4)\n",
    "\n",
    "    # save model's state_dict\n",
    "    fn = f\"{save_file_id if save_file_id else 'model'}_state_dict.pt\"\n",
    "    fp = os.path.join(config['model']['save_dir'],fn)\n",
    "    torch.save(model.state_dict(), fp)\n",
    "    \n",
    "    \n",
    "    return model, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_img_pred_gt(imgs[idx], tp[idx], tn[idx], figsize=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, modelPath):\n",
    "#     if not os.path.exists(\"results\"):\n",
    "#         os.makedirs(\"results\")\n",
    "#     model.load_state_dict(torch.load(modelPath))\n",
    "#     model.eval()\n",
    "#     model = model.to(device)\n",
    "#     df = pd.DataFrame(columns=classes)\n",
    "#     with torch.no_grad():\n",
    "#         for idx, sample in enumerate(dataloader_test):\n",
    "#             outputs = model(sample['image'].to(device))\n",
    "#             outputs = softmax(outputs)\n",
    "#             outputs = outputs.cpu().numpy()\n",
    "#             df = df.append(\n",
    "#                 pd.DataFrame(data=outputs, columns=classes))\n",
    "#         df = df.reset_index()\n",
    "#         del df['index']\n",
    "#         df.insert(0, 'image', images['image'])\n",
    "#     df.to_csv(f'results/test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     model = EfficientNet.from_pretrained('efficientnet-b3', num_classes=9)\n",
    "#     model, fileName = train_model(model, 16, 0.01, 1, 0, 0, '3')\n",
    "#     validate(model)\n",
    "#     test_model(model, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-bride",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(**config['model']['params'])\n",
    "torch.cuda.empty_cache()\n",
    "model = model.to(device)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "os.makedirs(config['model']['save_dir'], exist_ok=True)\n",
    "model_path = f\"{config['model']['save_dir']}/model_state_dict.pt\"\n",
    "\n",
    "if config['model']['load_weights']:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Loaded pre-trained weights...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-calgary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train(\n",
    "    model, \n",
    "    device, \n",
    "    tr_dataloader,\n",
    "    vl_dataloader,\n",
    "    config,\n",
    "    save_dir = config['model']['save_dir'],\n",
    "    save_file_id = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-ordering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda11",
   "language": "python",
   "name": "pytorch_cuda11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
